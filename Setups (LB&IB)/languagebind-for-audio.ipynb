{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up LanguageBind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/PKU-YuanGroup/LanguageBind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cd LanguageBind\n",
    "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r /kaggle/working/LanguageBind/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Audio Embeddings Without Clip Type\n",
    "**Audio Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from LanguageBind.languagebind import LanguageBindAudio, LanguageBindAudioTokenizer, LanguageBindAudioProcessor\n",
    "\n",
    "# Function to extract embeddings and save to CSV\n",
    "def extract_audio_embeddings(audio_folder: str, output_csv: str, batch_size=32):\n",
    "    # Load the pretrained model and tokenizer once\n",
    "    pretrained_ckpt = 'LanguageBind/LanguageBind_Audio_FT'\n",
    "    model = LanguageBindAudio.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir')\n",
    "    tokenizer = LanguageBindAudioTokenizer.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir')\n",
    "    audio_processor = LanguageBindAudioProcessor(model.config, tokenizer)\n",
    "\n",
    "    # Get all audio file paths\n",
    "    audio_files = [os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"No audio files found in the directory!\")\n",
    "        return\n",
    "\n",
    "    dummy_text = [\"\"]  # Single empty text string\n",
    "\n",
    "    # Put model in evaluation mode **before** the loop\n",
    "    model.eval()\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"Processing {total_files} audio files...\")\n",
    "\n",
    "    all_embeddings = []  # Store embeddings temporarily\n",
    "\n",
    "    for i in tqdm(range(0, total_files, batch_size), desc=\"Extracting Embeddings\"):\n",
    "        batch_files = audio_files[i:i + batch_size]\n",
    "\n",
    "        # Process the batch (reusing the single empty text input)\n",
    "        data = audio_processor(batch_files, dummy_text, return_tensors='pt')\n",
    "\n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**data)\n",
    "            audio_embeddings = outputs.image_embeds.cpu().numpy()  # Convert to NumPy array\n",
    "\n",
    "        # Store data in memory\n",
    "        for file, embedding in zip(batch_files, audio_embeddings):\n",
    "            all_embeddings.append([os.path.basename(file)] + embedding.tolist())\n",
    "\n",
    "    # Convert to DataFrame (after loop for efficiency)\n",
    "    df = pd.DataFrame(all_embeddings, columns=[\"filename\"] + [str(i) for i in range(768)])\n",
    "\n",
    "    # Save to CSV in one go\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete! Saved embeddings to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "extract_audio_embeddings(\"/home/iiitd/Nikhil/BTP_DATASET/audio_context\", \"lb_audio_context_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio Utterance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from LanguageBind.languagebind import LanguageBindAudio, LanguageBindAudioTokenizer, LanguageBindAudioProcessor\n",
    "\n",
    "# Function to extract embeddings and save to CSV\n",
    "def extract_audio_embeddings(audio_folder: str, output_csv: str, batch_size=32):\n",
    "    # Load the pretrained model and tokenizer once\n",
    "    pretrained_ckpt = 'LanguageBind/LanguageBind_Audio_FT'\n",
    "    model = LanguageBindAudio.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir')\n",
    "    tokenizer = LanguageBindAudioTokenizer.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir')\n",
    "    audio_processor = LanguageBindAudioProcessor(model.config, tokenizer)\n",
    "\n",
    "    # Get all audio file paths\n",
    "    audio_files = [os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"No audio files found in the directory!\")\n",
    "        return\n",
    "\n",
    "    dummy_text = [\"\"]  # Single empty text string\n",
    "\n",
    "    # Put model in evaluation mode **before** the loop\n",
    "    model.eval()\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"Processing {total_files} audio files...\")\n",
    "\n",
    "    all_embeddings = []  # Store embeddings temporarily\n",
    "\n",
    "    for i in tqdm(range(0, total_files, batch_size), desc=\"Extracting Embeddings\"):\n",
    "        batch_files = audio_files[i:i + batch_size]\n",
    "\n",
    "        # Process the batch (reusing the single empty text input)\n",
    "        data = audio_processor(batch_files, dummy_text, return_tensors='pt')\n",
    "\n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**data)\n",
    "            audio_embeddings = outputs.image_embeds.cpu().numpy()  # Convert to NumPy array\n",
    "\n",
    "        # Store data in memory\n",
    "        for file, embedding in zip(batch_files, audio_embeddings):\n",
    "            all_embeddings.append([os.path.basename(file)] + embedding.tolist())\n",
    "\n",
    "    # Convert to DataFrame (after loop for efficiency)\n",
    "    df = pd.DataFrame(all_embeddings, columns=[\"filename\"] + [str(i) for i in range(768)])\n",
    "\n",
    "    # Save to CSV in one go\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete! Saved embeddings to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "extract_audio_embeddings(\"/home/iiitd/Nikhil/BTP_DATASET/audio_utterance\", \"lb_audio_utterance_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Audio Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:38.769925Z",
     "iopub.status.busy": "2025-03-13T10:23:38.769703Z",
     "iopub.status.idle": "2025-03-13T10:23:42.057116Z",
     "shell.execute_reply": "2025-03-13T10:23:42.056195Z",
     "shell.execute_reply.started": "2025-03-13T10:23:38.769906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING LanguageBind Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/lb_audio_context_embeddings.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/lb_audio_utterance_embeddings.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_lb.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Audio Embeddings With Clip Type\n",
    "**Audio Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from LanguageBind.languagebind import LanguageBind, to_device, transform_dict, LanguageBindImageTokenizer\n",
    "\n",
    "def extract_audio_embeddings(audio_folder: str, output_csv: str, batch_size=32):\n",
    "    device = torch.device(\"cpu\")\n",
    "    clip_type = {'audio': 'LanguageBind_Audio_FT'}\n",
    "\n",
    "    model = LanguageBind(clip_type=clip_type, cache_dir='./cache_dir').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    pretrained_ckpt = 'lb203/LanguageBind_Image'\n",
    "    tokenizer = LanguageBindImageTokenizer.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir/tokenizer_cache_dir')\n",
    "    modality_transform = {c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()}\n",
    "\n",
    "    audio_files = [os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "    if not audio_files:\n",
    "        print(\"No audio files found in the directory!\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"Processing {total_files} audio files...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    language = [\"\"]\n",
    "\n",
    "    for i in tqdm(range(0, total_files, batch_size), desc=\"Extracting Embeddings\"):\n",
    "        batch_files = audio_files[i:i + batch_size]\n",
    "\n",
    "        inputs = {\n",
    "            'audio': to_device(modality_transform['audio'](batch_files), device),\n",
    "            'language': to_device(tokenizer(language, max_length=77, padding='max_length', truncation=True, return_tensors='pt'), device)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs)\n",
    "            audio_embeddings = embeddings['audio'].cpu().numpy()\n",
    "\n",
    "        for file, embedding in zip(batch_files, audio_embeddings):\n",
    "            all_embeddings.append([os.path.basename(file)] + embedding.tolist())\n",
    "\n",
    "    df = pd.DataFrame(all_embeddings, columns=[\"filename\"] + [str(i) for i in range(768)])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nProcessing complete! Saved embeddings to {output_csv}\")\n",
    "\n",
    "extract_audio_embeddings(\"/home/iiitd/Nikhil/BTP_DATASET/audio_context\", \"lb_audio_context_embeddings_cliptype.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio Utterance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from LanguageBind.languagebind import LanguageBind, to_device, transform_dict, LanguageBindImageTokenizer\n",
    "\n",
    "def extract_audio_embeddings(audio_folder: str, output_csv: str, batch_size=32):\n",
    "    device = torch.device(\"cpu\")\n",
    "    clip_type = {'audio': 'LanguageBind_Audio_FT'}\n",
    "\n",
    "    model = LanguageBind(clip_type=clip_type, cache_dir='./cache_dir').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    pretrained_ckpt = 'lb203/LanguageBind_Image'\n",
    "    tokenizer = LanguageBindImageTokenizer.from_pretrained(pretrained_ckpt, cache_dir='./cache_dir/tokenizer_cache_dir')\n",
    "    modality_transform = {c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()}\n",
    "\n",
    "    audio_files = [os.path.join(audio_folder, f) for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
    "    if not audio_files:\n",
    "        print(\"No audio files found in the directory!\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"Processing {total_files} audio files...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    language = [\"\"]\n",
    "\n",
    "    for i in tqdm(range(0, total_files, batch_size), desc=\"Extracting Embeddings\"):\n",
    "        batch_files = audio_files[i:i + batch_size]\n",
    "\n",
    "        inputs = {\n",
    "            'audio': to_device(modality_transform['audio'](batch_files), device),\n",
    "            'language': to_device(tokenizer(language, max_length=77, padding='max_length', truncation=True, return_tensors='pt'), device)\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs)\n",
    "            audio_embeddings = embeddings['audio'].cpu().numpy()\n",
    "\n",
    "        for file, embedding in zip(batch_files, audio_embeddings):\n",
    "            all_embeddings.append([os.path.basename(file)] + embedding.tolist())\n",
    "\n",
    "    df = pd.DataFrame(all_embeddings, columns=[\"filename\"] + [str(i) for i in range(768)])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nProcessing complete! Saved embeddings to {output_csv}\")\n",
    "\n",
    "extract_audio_embeddings(\"/home/iiitd/Nikhil/BTP_DATASET/audio_utterance\", \"lb_audio_utterance_embeddings_cliptype.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Audio features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T19:34:11.866355Z",
     "iopub.status.busy": "2025-03-14T19:34:11.866022Z",
     "iopub.status.idle": "2025-03-14T19:34:15.366060Z",
     "shell.execute_reply": "2025-03-14T19:34:15.365149Z",
     "shell.execute_reply.started": "2025-03-14T19:34:11.866330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING LanguageBind Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/lb_audio_context_embeddings_cliptype.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/lb_audio_utterance_embeddings_cliptype.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_lb_cliptype.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T08:04:34.235217Z",
     "iopub.status.busy": "2025-03-21T08:04:34.234903Z",
     "iopub.status.idle": "2025-03-21T08:04:44.077507Z",
     "shell.execute_reply": "2025-03-21T08:04:44.076764Z",
     "shell.execute_reply.started": "2025-03-21T08:04:34.235188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">766</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ reshape_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">766</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ reshape_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ max_pooling1d_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">383</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ max_pooling1d_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">383</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ flatten_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49024</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ flatten_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49024</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ concatenate_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98048</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ flatten_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,049</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_12 (\u001b[38;5;33mReshape\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ input_layer_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ reshape_13 (\u001b[38;5;33mReshape\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m766\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ reshape_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m766\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ reshape_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ max_pooling1d_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m383\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ max_pooling1d_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m383\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ flatten_12 (\u001b[38;5;33mFlatten\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49024\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ max_pooling1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ flatten_13 (\u001b[38;5;33mFlatten\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49024\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ max_pooling1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ concatenate_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98048\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ flatten_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ flatten_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m98,049\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,073</span> (387.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m99,073\u001b[0m (387.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99,073</span> (387.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m99,073\u001b[0m (387.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5702 - loss: 0.6850\n",
      "Epoch 1: val_accuracy improved from -inf to 0.70000, saving model to /kaggle/working/lb_cnn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.5712 - loss: 0.6847 - val_accuracy: 0.7000 - val_loss: 0.6520\n",
      "Epoch 2/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 0.6491 \n",
      "Epoch 2: val_accuracy did not improve from 0.70000\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6382 - loss: 0.6470 - val_accuracy: 0.6500 - val_loss: 0.6205\n",
      "Epoch 3/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6799 - loss: 0.6112 \n",
      "Epoch 3: val_accuracy did not improve from 0.70000\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6806 - loss: 0.6100 - val_accuracy: 0.6500 - val_loss: 0.6090\n",
      "Epoch 4/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6708 - loss: 0.6029 \n",
      "Epoch 4: val_accuracy did not improve from 0.70000\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6743 - loss: 0.6003 - val_accuracy: 0.7000 - val_loss: 0.5874\n",
      "Epoch 5/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6563 - loss: 0.5931 \n",
      "Epoch 5: val_accuracy did not improve from 0.70000\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6600 - loss: 0.5905 - val_accuracy: 0.7000 - val_loss: 0.5804\n",
      "Epoch 6/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7154 - loss: 0.5526 \n",
      "Epoch 6: val_accuracy improved from 0.70000 to 0.70833, saving model to /kaggle/working/lb_cnn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7135 - loss: 0.5542 - val_accuracy: 0.7083 - val_loss: 0.5757\n",
      "Epoch 7/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5348 \n",
      "Epoch 7: val_accuracy did not improve from 0.70833\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7431 - loss: 0.5378 - val_accuracy: 0.6583 - val_loss: 0.5816\n",
      "Epoch 8/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7043 - loss: 0.5323 \n",
      "Epoch 8: val_accuracy improved from 0.70833 to 0.72500, saving model to /kaggle/working/lb_cnn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7037 - loss: 0.5335 - val_accuracy: 0.7250 - val_loss: 0.5642\n",
      "Epoch 9/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7285 - loss: 0.5352 \n",
      "Epoch 9: val_accuracy improved from 0.72500 to 0.73333, saving model to /kaggle/working/lb_cnn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7300 - loss: 0.5335 - val_accuracy: 0.7333 - val_loss: 0.5554\n",
      "Epoch 10/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7300 - loss: 0.5208 \n",
      "Epoch 10: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7313 - loss: 0.5204 - val_accuracy: 0.7333 - val_loss: 0.5528\n",
      "Epoch 11/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7281 - loss: 0.5344 \n",
      "Epoch 11: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7305 - loss: 0.5303 - val_accuracy: 0.7000 - val_loss: 0.5635\n",
      "Epoch 12/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7527 - loss: 0.4932 \n",
      "Epoch 12: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7534 - loss: 0.4954 - val_accuracy: 0.7333 - val_loss: 0.5452\n",
      "Epoch 13/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7765 - loss: 0.4899 \n",
      "Epoch 13: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7740 - loss: 0.4907 - val_accuracy: 0.7083 - val_loss: 0.5482\n",
      "Epoch 14/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7592 - loss: 0.4960 \n",
      "Epoch 14: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7617 - loss: 0.4937 - val_accuracy: 0.6917 - val_loss: 0.5497\n",
      "Epoch 15/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7489 - loss: 0.4995 \n",
      "Epoch 15: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7539 - loss: 0.4945 - val_accuracy: 0.6917 - val_loss: 0.5522\n",
      "Epoch 16/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7772 - loss: 0.4782 \n",
      "Epoch 16: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7749 - loss: 0.4785 - val_accuracy: 0.7167 - val_loss: 0.5387\n",
      "Epoch 17/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7577 - loss: 0.4743 \n",
      "Epoch 17: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7626 - loss: 0.4718 - val_accuracy: 0.7083 - val_loss: 0.5501\n",
      "Epoch 18/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7943 - loss: 0.4477 \n",
      "Epoch 18: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7939 - loss: 0.4487 - val_accuracy: 0.7083 - val_loss: 0.5368\n",
      "Epoch 19/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7938 - loss: 0.4383 \n",
      "Epoch 19: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7936 - loss: 0.4401 - val_accuracy: 0.7083 - val_loss: 0.5369\n",
      "Epoch 20/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.4489 \n",
      "Epoch 20: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8067 - loss: 0.4477 - val_accuracy: 0.7083 - val_loss: 0.5358\n",
      "Epoch 21/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8258 - loss: 0.4103 \n",
      "Epoch 21: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8248 - loss: 0.4146 - val_accuracy: 0.6917 - val_loss: 0.5585\n",
      "Epoch 22/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8454 - loss: 0.4238 \n",
      "Epoch 22: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8429 - loss: 0.4224 - val_accuracy: 0.7000 - val_loss: 0.5454\n",
      "Epoch 23/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8220 - loss: 0.4161 \n",
      "Epoch 23: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8231 - loss: 0.4163 - val_accuracy: 0.7000 - val_loss: 0.5344\n",
      "Epoch 24/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8458 - loss: 0.3796 \n",
      "Epoch 24: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8401 - loss: 0.3863 - val_accuracy: 0.6917 - val_loss: 0.5562\n",
      "Epoch 25/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8427 - loss: 0.4055 \n",
      "Epoch 25: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8417 - loss: 0.4056 - val_accuracy: 0.7333 - val_loss: 0.5778\n",
      "Epoch 26/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8551 - loss: 0.3769 \n",
      "Epoch 26: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8534 - loss: 0.3807 - val_accuracy: 0.7167 - val_loss: 0.5409\n",
      "Epoch 27/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.3950 \n",
      "Epoch 27: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8443 - loss: 0.3935 - val_accuracy: 0.6833 - val_loss: 0.5455\n",
      "Epoch 28/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.3756 \n",
      "Epoch 28: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8545 - loss: 0.3757 - val_accuracy: 0.6833 - val_loss: 0.5484\n",
      "Epoch 29/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8527 - loss: 0.3663 \n",
      "Epoch 29: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8534 - loss: 0.3673 - val_accuracy: 0.6833 - val_loss: 0.5618\n",
      "Epoch 30/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.3676 \n",
      "Epoch 30: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8559 - loss: 0.3675 - val_accuracy: 0.7083 - val_loss: 0.5434\n",
      "Epoch 31/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3613 \n",
      "Epoch 31: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.3617 - val_accuracy: 0.6917 - val_loss: 0.5713\n",
      "Epoch 32/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3568 \n",
      "Epoch 32: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8733 - loss: 0.3561 - val_accuracy: 0.6917 - val_loss: 0.5742\n",
      "Epoch 33/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9023 - loss: 0.3231 \n",
      "Epoch 33: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8983 - loss: 0.3280 - val_accuracy: 0.6917 - val_loss: 0.5627\n",
      "Epoch 34/50\n",
      "\u001b[1m19/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8827 - loss: 0.3260 \n",
      "Epoch 34: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.3305 - val_accuracy: 0.7083 - val_loss: 0.5628\n",
      "Epoch 35/50\n",
      "\u001b[1m20/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9059 - loss: 0.2989 \n",
      "Epoch 35: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8991 - loss: 0.3084 - val_accuracy: 0.7000 - val_loss: 0.5509\n",
      "Epoch 36/50\n",
      "\u001b[1m19/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3160 \n",
      "Epoch 36: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8844 - loss: 0.3209 - val_accuracy: 0.7083 - val_loss: 0.5556\n",
      "Epoch 37/50\n",
      "\u001b[1m20/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8933 - loss: 0.3161 \n",
      "Epoch 37: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8928 - loss: 0.3186 - val_accuracy: 0.7000 - val_loss: 0.5767\n",
      "Epoch 38/50\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9017 - loss: 0.3034 \n",
      "Epoch 38: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.3052 - val_accuracy: 0.6750 - val_loss: 0.5869\n",
      "Epoch 39/50\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9126 - loss: 0.2902 \n",
      "Epoch 39: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.2948 - val_accuracy: 0.6833 - val_loss: 0.5802\n",
      "Epoch 40/50\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8818 - loss: 0.3149 \n",
      "Epoch 40: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8851 - loss: 0.3121 - val_accuracy: 0.6917 - val_loss: 0.5844\n",
      "Epoch 41/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9159 - loss: 0.2787 \n",
      "Epoch 41: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9132 - loss: 0.2819 - val_accuracy: 0.6833 - val_loss: 0.5732\n",
      "Epoch 42/50\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9090 - loss: 0.2858 \n",
      "Epoch 42: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2858 - val_accuracy: 0.6750 - val_loss: 0.5928\n",
      "Epoch 43/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9095 - loss: 0.2912 \n",
      "Epoch 43: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9088 - loss: 0.2895 - val_accuracy: 0.6583 - val_loss: 0.5905\n",
      "Epoch 44/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9188 - loss: 0.2773 \n",
      "Epoch 44: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9173 - loss: 0.2777 - val_accuracy: 0.6833 - val_loss: 0.5902\n",
      "Epoch 45/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8988 - loss: 0.2891 \n",
      "Epoch 45: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8998 - loss: 0.2872 - val_accuracy: 0.7083 - val_loss: 0.6409\n",
      "Epoch 46/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9285 - loss: 0.2636 \n",
      "Epoch 46: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9271 - loss: 0.2638 - val_accuracy: 0.6500 - val_loss: 0.6059\n",
      "Epoch 47/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9179 - loss: 0.2649 \n",
      "Epoch 47: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9181 - loss: 0.2641 - val_accuracy: 0.6583 - val_loss: 0.6254\n",
      "Epoch 48/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9232 - loss: 0.2509 \n",
      "Epoch 48: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2522 - val_accuracy: 0.6667 - val_loss: 0.6162\n",
      "Epoch 49/50\n",
      "\u001b[1m22/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.2611 \n",
      "Epoch 49: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9049 - loss: 0.2603 - val_accuracy: 0.6833 - val_loss: 0.6057\n",
      "Epoch 50/50\n",
      "\u001b[1m23/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2408 \n",
      "Epoch 50: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9283 - loss: 0.2405 - val_accuracy: 0.7083 - val_loss: 0.6854\n",
      "Loaded Best Model Weights.\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Train Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7660    0.6857    0.7236       420\n",
      "         1.0     0.7161    0.7910    0.7517       421\n",
      "\n",
      "    accuracy                         0.7384       841\n",
      "   macro avg     0.7410    0.7383    0.7377       841\n",
      "weighted avg     0.7410    0.7384    0.7377       841\n",
      "\n",
      "Validation Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7333    0.7333    0.7333        60\n",
      "         1.0     0.7333    0.7333    0.7333        60\n",
      "\n",
      "    accuracy                         0.7333       120\n",
      "   macro avg     0.7333    0.7333    0.7333       120\n",
      "weighted avg     0.7333    0.7333    0.7333       120\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7429    0.6446    0.6903       121\n",
      "         1.0     0.6838    0.7750    0.7266       120\n",
      "\n",
      "    accuracy                         0.7095       241\n",
      "   macro avg     0.7133    0.7098    0.7084       241\n",
      "weighted avg     0.7135    0.7095    0.7083       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN MODEL\n",
    "\"\"\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the final dataset\n",
    "df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_features_lb.csv\")\n",
    "\n",
    "# Extract labels\n",
    "y = df[\"Sarcasm\"].values  # Labels (0: No sarcasm, 1: Sarcasm)\n",
    "\n",
    "# Extract context features (from csv1_)\n",
    "X_context = df[[col for col in df.columns if col.startswith(\"audio_c_feature_\")]].values\n",
    "\n",
    "# Extract utterance features (from csv2_)\n",
    "X_utterance = df[[col for col in df.columns if col.startswith(\"audio_u_feature_\")]].values\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_context = np.array(X_context, dtype=np.float32)\n",
    "X_utterance = np.array(X_utterance, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# First, split into train (70%) and temp (30%) \n",
    "Xc_train, Xc_temp, Xu_train, Xu_temp, y_train, y_temp = train_test_split(\n",
    "    X_context, X_utterance, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split temp (30%) into validation (10%) and test (20%)\n",
    "Xc_val, Xc_test, Xu_val, Xu_test, y_val, y_test = train_test_split(\n",
    "    Xc_temp, Xu_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# CNN Model for Sarcasm Detection\n",
    "input_dim = 768  # Number of features per input\n",
    "\n",
    "# Context Branch\n",
    "input_context = keras.Input(shape=(input_dim,))\n",
    "context_branch = layers.Reshape((input_dim, 1))(input_context)\n",
    "context_branch = layers.Conv1D(filters=128, kernel_size=3, activation=\"swish\")(context_branch)\n",
    "context_branch = layers.MaxPooling1D(pool_size=2)(context_branch)\n",
    "context_branch = layers.Flatten()(context_branch)\n",
    "\n",
    "# Utterance Branch\n",
    "input_utterance = keras.Input(shape=(input_dim,))\n",
    "utterance_branch = layers.Reshape((input_dim, 1))(input_utterance)\n",
    "utterance_branch = layers.Conv1D(filters=128, kernel_size=3, activation=\"swish\")(utterance_branch)\n",
    "utterance_branch = layers.MaxPooling1D(pool_size=2)(utterance_branch)\n",
    "utterance_branch = layers.Flatten()(utterance_branch)\n",
    "\n",
    "# Concatenation\n",
    "merged = layers.Concatenate()([context_branch, utterance_branch])\n",
    "#merged = layers.Dense(768, activation=\"relu\")(merged)\n",
    "#merged = layers.Dense(32, activation=\"swish\")(merged)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(merged)  # Sigmoid for binary classification\n",
    "\n",
    "# Define Model\n",
    "model = keras.Model(inputs=[input_context, input_utterance], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Checkpoint to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"/kaggle/working/lb_cnn_model.weights.h5\",\n",
    "    monitor=\"val_accuracy\",  # Monitor validation accuracy\n",
    "    mode=\"max\",  # Save when val_accuracy is maximum\n",
    "    save_best_only=True,  # Keep only the best weights\n",
    "    save_weights_only=True,  # Don't save full model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "model.fit(\n",
    "    [Xc_train, Xu_train], y_train,\n",
    "    epochs=50, batch_size=32,\n",
    "    validation_data=([Xc_val, Xu_val], y_val),  # Use validation set\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_weights(\"/kaggle/working/lb_cnn_model.weights.h5\")\n",
    "print(\"Loaded Best Model Weights.\")\n",
    "\n",
    "# Generate predictions using the best model\n",
    "y_train_pred = (model.predict([Xc_train, Xu_train]) > 0.5).astype(int)\n",
    "y_val_pred = (model.predict([Xc_val, Xu_val]) > 0.5).astype(int)\n",
    "y_test_pred = (model.predict([Xc_test, Xu_test]) > 0.5).astype(int)\n",
    "\n",
    "# Print classification reports for all sets\n",
    "print(\"Train Set Classification Report:\\n\", classification_report(y_train, y_train_pred,digits=4))\n",
    "print(\"Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred,digits=4))\n",
    "print(\"Test Set Classification Report:\\n\", classification_report(y_test, y_test_pred,digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T08:14:43.028489Z",
     "iopub.status.busy": "2025-03-21T08:14:43.028185Z",
     "iopub.status.idle": "2025-03-21T08:14:50.245476Z",
     "shell.execute_reply": "2025-03-21T08:14:50.244654Z",
     "shell.execute_reply.started": "2025-03-21T08:14:43.028464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m196,736\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">196,865</span> (769.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m196,865\u001b[0m (769.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">196,865</span> (769.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m196,865\u001b[0m (769.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5388 - loss: 1.3863\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65833, saving model to /kaggle/working/lb_fcn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5405 - loss: 1.3714 - val_accuracy: 0.6583 - val_loss: 0.5870\n",
      "Epoch 2/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5938 - loss: 0.6341\n",
      "Epoch 2: val_accuracy improved from 0.65833 to 0.68333, saving model to /kaggle/working/lb_fcn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6676 - loss: 0.5740 - val_accuracy: 0.6833 - val_loss: 0.5531\n",
      "Epoch 3/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8750 - loss: 0.4641\n",
      "Epoch 3: val_accuracy improved from 0.68333 to 0.71667, saving model to /kaggle/working/lb_fcn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.4968 - val_accuracy: 0.7167 - val_loss: 0.5405\n",
      "Epoch 4/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8438 - loss: 0.5181\n",
      "Epoch 4: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.4487 - val_accuracy: 0.7083 - val_loss: 0.5468\n",
      "Epoch 5/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8125 - loss: 0.3828\n",
      "Epoch 5: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7900 - loss: 0.4385 - val_accuracy: 0.6667 - val_loss: 0.5827\n",
      "Epoch 6/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7500 - loss: 0.4899\n",
      "Epoch 6: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8128 - loss: 0.4014 - val_accuracy: 0.6833 - val_loss: 0.6179\n",
      "Epoch 7/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8438 - loss: 0.3380\n",
      "Epoch 7: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8542 - loss: 0.3580 - val_accuracy: 0.6917 - val_loss: 0.7781\n",
      "Epoch 8/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8125 - loss: 0.4065\n",
      "Epoch 8: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3428 - val_accuracy: 0.7000 - val_loss: 0.5998\n",
      "Epoch 9/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9062 - loss: 0.2549\n",
      "Epoch 9: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8693 - loss: 0.3049 - val_accuracy: 0.7167 - val_loss: 0.5726\n",
      "Epoch 10/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8750 - loss: 0.3200\n",
      "Epoch 10: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9109 - loss: 0.2552 - val_accuracy: 0.7000 - val_loss: 0.6263\n",
      "Epoch 11/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9062 - loss: 0.2403\n",
      "Epoch 11: val_accuracy did not improve from 0.71667\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9204 - loss: 0.2326 - val_accuracy: 0.7167 - val_loss: 0.7670\n",
      "Epoch 12/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8750 - loss: 0.2723\n",
      "Epoch 12: val_accuracy improved from 0.71667 to 0.73333, saving model to /kaggle/working/lb_fcn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.2452 - val_accuracy: 0.7333 - val_loss: 0.6739\n",
      "Epoch 13/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9375 - loss: 0.1909\n",
      "Epoch 13: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1702 - val_accuracy: 0.7000 - val_loss: 0.7055\n",
      "Epoch 14/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: 0.1995\n",
      "Epoch 14: val_accuracy did not improve from 0.73333\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9525 - loss: 0.1652 - val_accuracy: 0.7083 - val_loss: 0.8456\n",
      "Epoch 15/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0931\n",
      "Epoch 15: val_accuracy improved from 0.73333 to 0.74167, saving model to /kaggle/working/lb_fcn_model.weights.h5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.1230 - val_accuracy: 0.7417 - val_loss: 0.7320\n",
      "Epoch 16/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9375 - loss: 0.1612\n",
      "Epoch 16: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9619 - loss: 0.1352 - val_accuracy: 0.6833 - val_loss: 0.8401\n",
      "Epoch 17/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0570\n",
      "Epoch 17: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0855 - val_accuracy: 0.6500 - val_loss: 1.0790\n",
      "Epoch 18/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: 0.1303\n",
      "Epoch 18: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0998 - val_accuracy: 0.7333 - val_loss: 0.7161\n",
      "Epoch 19/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0343\n",
      "Epoch 19: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0594 - val_accuracy: 0.6917 - val_loss: 0.8120\n",
      "Epoch 20/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0607\n",
      "Epoch 20: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0519 - val_accuracy: 0.6750 - val_loss: 0.8903\n",
      "Epoch 21/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0374\n",
      "Epoch 21: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0498 - val_accuracy: 0.7000 - val_loss: 0.8886\n",
      "Epoch 22/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0440\n",
      "Epoch 22: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0417 - val_accuracy: 0.6833 - val_loss: 0.9022\n",
      "Epoch 23/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0334\n",
      "Epoch 23: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0310 - val_accuracy: 0.6750 - val_loss: 0.8959\n",
      "Epoch 24/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0250\n",
      "Epoch 24: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0276 - val_accuracy: 0.7083 - val_loss: 0.8373\n",
      "Epoch 25/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0361\n",
      "Epoch 25: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.6833 - val_loss: 0.8878\n",
      "Epoch 26/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0126\n",
      "Epoch 26: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0278 - val_accuracy: 0.6917 - val_loss: 0.9450\n",
      "Epoch 27/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0100\n",
      "Epoch 27: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 0.6917 - val_loss: 0.9089\n",
      "Epoch 28/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0158\n",
      "Epoch 28: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0144 - val_accuracy: 0.6917 - val_loss: 0.9747\n",
      "Epoch 29/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0149\n",
      "Epoch 29: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 0.6833 - val_loss: 1.0053\n",
      "Epoch 30/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0111\n",
      "Epoch 30: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.6833 - val_loss: 0.9483\n",
      "Epoch 31/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0079\n",
      "Epoch 31: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.6833 - val_loss: 0.9514\n",
      "Epoch 32/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0100\n",
      "Epoch 32: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 0.6917 - val_loss: 1.0322\n",
      "Epoch 33/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0077\n",
      "Epoch 33: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.6833 - val_loss: 1.0596\n",
      "Epoch 34/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0071\n",
      "Epoch 34: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.6750 - val_loss: 0.9773\n",
      "Epoch 35/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0084\n",
      "Epoch 35: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.6917 - val_loss: 1.0452\n",
      "Epoch 36/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0077\n",
      "Epoch 36: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.6833 - val_loss: 1.0618\n",
      "Epoch 37/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0057\n",
      "Epoch 37: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.6750 - val_loss: 1.0317\n",
      "Epoch 38/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0077\n",
      "Epoch 38: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.6667 - val_loss: 1.0702\n",
      "Epoch 39/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0057\n",
      "Epoch 39: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.6750 - val_loss: 1.0857\n",
      "Epoch 40/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 40: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.6833 - val_loss: 1.0864\n",
      "Epoch 41/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 41: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6917 - val_loss: 1.0532\n",
      "Epoch 42/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 42: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.6750 - val_loss: 1.1245\n",
      "Epoch 43/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Epoch 43: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6667 - val_loss: 1.0806\n",
      "Epoch 44/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 44: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.6667 - val_loss: 1.1238\n",
      "Epoch 45/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 45: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.6917 - val_loss: 1.0467\n",
      "Epoch 46/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 46: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.6667 - val_loss: 1.1081\n",
      "Epoch 47/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 47: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.6667 - val_loss: 1.1234\n",
      "Epoch 48/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 48: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6750 - val_loss: 1.2051\n",
      "Epoch 49/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 49: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6833 - val_loss: 1.1651\n",
      "Epoch 50/50\n",
      "\u001b[1m 1/27\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 50: val_accuracy did not improve from 0.74167\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6583 - val_loss: 1.1465\n",
      "Loaded Best Model Weights.\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Train Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    0.8952    0.9447       420\n",
      "         1.0     0.9054    1.0000    0.9503       421\n",
      "\n",
      "    accuracy                         0.9477       841\n",
      "   macro avg     0.9527    0.9476    0.9475       841\n",
      "weighted avg     0.9526    0.9477    0.9475       841\n",
      "\n",
      "Validation Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7736    0.6833    0.7257        60\n",
      "         1.0     0.7164    0.8000    0.7559        60\n",
      "\n",
      "    accuracy                         0.7417       120\n",
      "   macro avg     0.7450    0.7417    0.7408       120\n",
      "weighted avg     0.7450    0.7417    0.7408       120\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7849    0.6033    0.6822       121\n",
      "         1.0     0.6757    0.8333    0.7463       120\n",
      "\n",
      "    accuracy                         0.7178       241\n",
      "   macro avg     0.7303    0.7183    0.7143       241\n",
      "weighted avg     0.7305    0.7178    0.7141       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "FCN MODEL\n",
    "\"\"\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the final dataset\n",
    "df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_features_lb_cliptype.csv\")\n",
    "\n",
    "# Extract labels\n",
    "y = df[\"Sarcasm\"].values  # Labels (0: No sarcasm, 1: Sarcasm)\n",
    "\n",
    "# Extract context and utterance features\n",
    "X_context = df[[col for col in df.columns if col.startswith(\"audio_c_feature_\")]].values\n",
    "X_utterance = df[[col for col in df.columns if col.startswith(\"audio_u_feature_\")]].values\n",
    "\n",
    "# Concatenate context and utterance embeddings\n",
    "X = np.concatenate((X_context, X_utterance), axis=1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# First, split into train (70%) and temp (30%) \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split temp (30%) into validation (10%) and test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Define FCN Model\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation=\"swish\", input_shape=(input_dim,)),\n",
    "    #layers.BatchNormalization(),\n",
    "    #layers.Dropout(0.3),\n",
    "    #layers.Dense(256, activation=\"relu\"),\n",
    "    #layers.BatchNormalization(),\n",
    "    #layers.Dropout(0.3),\n",
    "    #layers.Dense(128, activation=\"relu\"),\n",
    "    #layers.BatchNormalization(),\n",
    "    #layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\")  # Binary classification\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Checkpoint to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"/kaggle/working/lb_fcn_model.weights.h5\",\n",
    "    monitor=\"val_accuracy\",  # Monitor validation accuracy\n",
    "    mode=\"max\",  # Save when val_accuracy is maximum\n",
    "    save_best_only=True,  # Keep only the best weights\n",
    "    save_weights_only=True,  # Don't save full model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50, batch_size=32,\n",
    "    validation_data=(X_val, y_val),  # Use validation set\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_weights(\"/kaggle/working/lb_fcn_model.weights.h5\")\n",
    "print(\"Loaded Best Model Weights.\")\n",
    "\n",
    "# Generate predictions using the best model\n",
    "y_train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "y_val_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Print classification reports for all sets\n",
    "print(\"Train Set Classification Report:\\n\", classification_report(y_train, y_train_pred,digits=4))\n",
    "print(\"Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred,digits=4))\n",
    "print(\"Test Set Classification Report:\\n\", classification_report(y_test, y_test_pred,digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6848937,
     "sourceId": 11040239,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

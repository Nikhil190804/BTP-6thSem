{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Audio Embeddings From PTMs\n",
    "\n",
    "### Using A Wrapper by [Girish](https://github.com/CodeVault-girish/SFM-models.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-11T07:51:04.886159Z",
     "iopub.status.busy": "2025-03-11T07:51:04.885780Z",
     "iopub.status.idle": "2025-03-11T07:51:05.824405Z",
     "shell.execute_reply": "2025-03-11T07:51:05.823284Z",
     "shell.execute_reply.started": "2025-03-11T07:51:04.886127Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SFM-models'...\n",
      "remote: Enumerating objects: 164, done.\u001b[K\n",
      "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
      "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
      "remote: Total 164 (delta 86), reused 118 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (164/164), 28.74 KiB | 3.59 MiB/s, done.\n",
      "Resolving deltas: 100% (86/86), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CodeVault-girish/SFM-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:03:45.658457Z",
     "iopub.status.busy": "2025-03-21T18:03:45.658050Z",
     "iopub.status.idle": "2025-03-21T18:03:45.662027Z",
     "shell.execute_reply": "2025-03-21T18:03:45.661316Z",
     "shell.execute_reply.started": "2025-03-21T18:03:45.658424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/SFM-models\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:03:48.914036Z",
     "iopub.status.busy": "2025-03-21T18:03:48.913706Z",
     "iopub.status.idle": "2025-03-21T18:03:48.923129Z",
     "shell.execute_reply": "2025-03-21T18:03:48.922295Z",
     "shell.execute_reply.started": "2025-03-21T18:03:48.914011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "1. Trillsson\n",
      "2. YAMNet\n",
      "3. Facebook MMS-1B\n",
      "4. SpeechBrain x-vector\n",
      "5. Facebook HuBERT-base-ls960\n",
      "6. Microsoft WavLM-base\n",
      "7. Facebook Wav2Vec2-XLS-R-1B\n",
      "8. Facebook Wav2Vec2-base\n",
      "9. OpenAI Whisper-base\n",
      "10. Microsoft UniSpeech-SAT-base-100h-Libri-ft\n",
      "11. speechbrain/spkrec-ecapa-voxceleb\n"
     ]
    }
   ],
   "source": [
    "from sfm_extractor.extractor import model_list, extract_from\n",
    "model_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Specified Files for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:08:43.194044Z",
     "iopub.status.busy": "2025-03-21T18:08:43.193700Z",
     "iopub.status.idle": "2025-03-21T18:08:43.768318Z",
     "shell.execute_reply": "2025-03-21T18:08:43.767097Z",
     "shell.execute_reply.started": "2025-03-21T18:08:43.194014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls\n",
    "# !touch audio_context_WavLM_base_embeddings.csv\n",
    "# !touch audio_utterance_WavLM_base_embeddings.csv\n",
    "# !touch audio_utterance_Wav2Vec2_base_embeddings.csv\n",
    "# !touch audio_context_Wav2Vec2_base_embeddings.csv\n",
    "!touch audio_context_mms_embeddings.csv\n",
    "!touch audio_utterance_mms_embeddings.csv\n",
    "!touch audio_context_hubert_embeddings.csv\n",
    "!touch audio_utterance_hubert_embeddings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Various Embeddings From PTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:08:47.235702Z",
     "iopub.status.busy": "2025-03-21T18:08:47.235348Z",
     "iopub.status.idle": "2025-03-21T18:20:43.879526Z",
     "shell.execute_reply": "2025-03-21T18:20:43.878340Z",
     "shell.execute_reply.started": "2025-03-21T18:08:47.235672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [11:52<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_context_mms_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"3\", \"/kaggle/input/btp-dataset/audio_context/audio_context\", output_file=\"/kaggle/working/audio_context_mms_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:44:38.964502Z",
     "iopub.status.busy": "2025-03-21T18:44:38.964063Z",
     "iopub.status.idle": "2025-03-21T18:49:10.817112Z",
     "shell.execute_reply": "2025-03-21T18:49:10.816134Z",
     "shell.execute_reply.started": "2025-03-21T18:44:38.964467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [04:26<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_utterance_mms_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"3\", \"/kaggle/input/btp-dataset/audio_utterance/audio_utterance\", output_file=\"/kaggle/working/audio_utterance_mms_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:49:13.652816Z",
     "iopub.status.busy": "2025-03-21T18:49:13.652486Z",
     "iopub.status.idle": "2025-03-21T18:50:12.463362Z",
     "shell.execute_reply": "2025-03-21T18:50:12.462322Z",
     "shell.execute_reply.started": "2025-03-21T18:49:13.652792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73321f9b19c6458da4f9ed19425afc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660a30f21c86432693ff7d3a17bbac54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e79aa391934537b26091adab918877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 1202/1202 [00:54<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_utterance_hubert_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"5\", \"/kaggle/input/btp-dataset/audio_utterance/audio_utterance\", output_file=\"/kaggle/working/audio_utterance_hubert_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:50:38.964830Z",
     "iopub.status.busy": "2025-03-21T18:50:38.964496Z",
     "iopub.status.idle": "2025-03-21T18:53:01.249448Z",
     "shell.execute_reply": "2025-03-21T18:53:01.248504Z",
     "shell.execute_reply.started": "2025-03-21T18:50:38.964801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 1202/1202 [02:20<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_context_hubert_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"5\", \"/kaggle/input/btp-dataset/audio_context/audio_context\", output_file=\"/kaggle/working/audio_context_hubert_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T08:43:58.381447Z",
     "iopub.status.busy": "2025-03-11T08:43:58.381184Z",
     "iopub.status.idle": "2025-03-11T08:45:01.235866Z",
     "shell.execute_reply": "2025-03-11T08:45:01.234976Z",
     "shell.execute_reply.started": "2025-03-11T08:43:58.381419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ef0d4f4cea4ed091587b61a0333c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412b14f7b4584644b608f878be2c97de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5704e0062ba04999a01f0df1213fe847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 1202/1202 [00:58<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_utterance_Wav2Vec2_base_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"8\", \"/kaggle/input/btp-dataset/audio_utterance/audio_utterance\", output_file=\"/kaggle/working/audio_utterance_Wav2Vec2_base_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T08:45:01.238416Z",
     "iopub.status.busy": "2025-03-11T08:45:01.238147Z",
     "iopub.status.idle": "2025-03-11T08:47:34.736312Z",
     "shell.execute_reply": "2025-03-11T08:47:34.735269Z",
     "shell.execute_reply.started": "2025-03-11T08:45:01.238386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Processing audio files: 100%|██████████| 1202/1202 [02:31<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all features to /kaggle/working/audio_context_Wav2Vec2_base_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "extract_from(\"8\", \"/kaggle/input/btp-dataset/audio_context/audio_context\", output_file=\"/kaggle/working/audio_context_Wav2Vec2_base_embeddings.csv\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Audio Embeddings (Audio Context + Audio Utterance -> Audio Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING WavLM Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_context_WavLM_base_embeddings.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_utterance_WavLM_base_embeddings.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_WavLM_base.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as final_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING Wav2Vec2 Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_context_Wav2Vec2_base_embeddings.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/input/btp-audioembeddings/audio_utterance_Wav2Vec2_base_embeddings.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_Wav2Vec2_base.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as final_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T18:57:23.453531Z",
     "iopub.status.busy": "2025-03-21T18:57:23.453113Z",
     "iopub.status.idle": "2025-03-21T18:57:25.740616Z",
     "shell.execute_reply": "2025-03-21T18:57:25.739788Z",
     "shell.execute_reply.started": "2025-03-21T18:57:23.453499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as final_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING HUBBERT Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/working/audio_context_hubert_embeddings.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/working/audio_utterance_hubert_embeddings.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_hubert.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as final_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T19:00:23.513676Z",
     "iopub.status.busy": "2025-03-21T19:00:23.513292Z",
     "iopub.status.idle": "2025-03-21T19:00:27.122132Z",
     "shell.execute_reply": "2025-03-21T19:00:27.121307Z",
     "shell.execute_reply.started": "2025-03-21T19:00:23.513649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as final_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MERGING THE AUDIO EMBEDDINGS OF CONTEXT AND UTTERNACE WITH LABELS AND OTHER FEATURES\n",
    "MERGING HUBBERT Embeddings\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(\"/kaggle/working/audio_context_mms_embeddings.csv\")\n",
    "csv2 = pd.read_csv(\"/kaggle/working/audio_utterance_mms_embeddings.csv\")\n",
    "map_df = pd.read_csv(\"/kaggle/input/btp-audioembeddings/context_to_utterance_map.csv\")\n",
    "\n",
    "# Remove the 'audio_context/' and 'audio_utterance/' prefixes from map.csv\n",
    "map_df[\"audio_context\"] = map_df[\"audio_context\"].str.replace(\"audio_context/\", \"\", regex=False)\n",
    "map_df[\"audio_utterance\"] = map_df[\"audio_utterance\"].str.replace(\"audio_utterance/\", \"\", regex=False)\n",
    "\n",
    "# Extract features (excluding the first column which is file_name)\n",
    "features_csv1 = csv1.iloc[:, 1:].copy()  # Features from csv1\n",
    "features_csv2 = csv2.iloc[:, 1:].copy()  # Features from csv2\n",
    "\n",
    "# Rename columns to distinguish between csv1 and csv2 features\n",
    "features_csv1.columns = [f\"audio_c_feature_{col}\" for col in features_csv1.columns]\n",
    "features_csv2.columns = [f\"audio_u_feature_{col}\" for col in features_csv2.columns]\n",
    "\n",
    "# Add file_name back to features for merging\n",
    "features_csv1.insert(0, \"filename\", csv1.iloc[:, 0])\n",
    "features_csv2.insert(0, \"filename\", csv2.iloc[:, 0])\n",
    "\n",
    "# Merge csv1 with map.csv using audio_context (which is file_name in csv1)\n",
    "merged_df = map_df.merge(features_csv1, left_on=\"audio_context\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Merge csv2 with the updated dataframe using audio_utterance (which is file_name in csv2)\n",
    "merged_df = merged_df.merge(features_csv2, left_on=\"audio_utterance\", right_on=\"filename\", how=\"inner\", suffixes=(\"_csv1\", \"_csv2\"))\n",
    "\n",
    "# Drop redundant filename columns from csv1 and csv2\n",
    "merged_df.drop(columns=[\"filename_csv1\", \"filename_csv2\"], inplace=True)\n",
    "\n",
    "# Rename columns to keep them organized\n",
    "#merged_df.rename(columns={\"audio_context\": \"file_csv1\", \"audio_utterance\": \"file_csv2\"}, inplace=True)\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(\"audio_features_mms.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as final_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6837116,
     "sourceId": 10990039,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6848937,
     "sourceId": 11040239,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
